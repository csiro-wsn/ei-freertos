#!/usr/bin/env python3
"""
Simple TDF Stream to CSV file
Duplicates the output format of sd2csv
"""

__author__ = "Jordan Yates"

import argparse
import datetime
import os

import tdf3
import ListenClient

from PacpMessage import PayloadType, DecryptionError
from NodeFilter import NodeFilter


def tdf3csv(base_host, base_port, tdf_server, output_path, date_format, filter_config, filter_strict, debug):
    tdfparse = tdf3.Tdf()
    tdfparse.loadTdf(tdf_server, timeout=10.0)
    parser = tdfparse.parseTdf16

    listener = ListenClient.ListenClient(base_host, base_port)
    listener.connect()

    node_filter = NodeFilter(filter_config)
    node_csvs = {}

    try:
        while True:
            try:
                packet = listener.read(timeout=None)
            except ConnectionResetError:
                print("Connection to baselisten lost...")
                break
            except NotImplementedError:
                continue

            for payload_type, route, payload in packet.iter_payloads():

                if payload_type == PayloadType.PAYLOAD_ENC_TDF3:
                    # Only try if there are encryption key
                    if node_filter.encryption_keys == []:
                        continue
                    # Attempt decryption using filter keys
                    for key in node_filter.encryption_keys:
                        try:
                            route, payload = packet.decrypt(route, payload, key)
                        except DecryptionError:
                            continue
                        break
                elif payload_type != PayloadType.PAYLOAD_TDF3:
                    continue

                first_hop = route[-1]
                node_fmt = "{:012X}".format(first_hop.address)

                if first_hop.address not in node_csvs:
                    node_csvs[first_hop.address] = {}

                try:
                    for sensor in parser(payload, time=datetime.datetime.utcnow(), debug=debug, combine=True):
                        # Apply device filtering rules
                        node_filter.consume_tdf(first_hop.address, sensor)
                        filter_result = node_filter.filter(first_hop.address)
                        if filter_strict and filter_result != NodeFilter.PASSED:
                            continue
                        elif filter_result == NodeFilter.FAILED:
                            continue

                        sensor_name = sensor['sensor']
                        # Check if sensor readings exists as a file
                        if sensor_name not in node_csvs[first_hop.address]:
                            filepath = os.path.join(output_path, "{:s}.{:s}.csv".format(node_fmt, sensor_name))
                            filepath_exists = os.path.isfile(filepath)
                            print("Opening {:s} file {:s}".format("existing" if filepath_exists else "new", filepath))
                            node_csvs[first_hop.address][sensor_name] = open(filepath, "a+")
                            # Write headings if this is a new file
                            if not filepath_exists:
                                node_csvs[first_hop.address][sensor_name].write("time,{:s}\n".format(','.join(sensor['phenomena'].keys())))
                        # Write the data
                        t = sensor['time'].strftime(date_format)
                        readings = [str(p['converted']) for p in sensor['phenomena'].values()]
                        node_csvs[first_hop.address][sensor_name].write("{:s},{:s}\n".format(t, ','.join(readings)))
                        node_csvs[first_hop.address][sensor_name].flush()

                except (tdf3.TdfLookupError, tdf3.TdfBufferLengthError) as e:
                    if debug:
                        print("Node: {:} = {}".format(node_fmt, e))
                    continue

    except KeyboardInterrupt:
        pass
    finally:
        for nodes in node_csvs.values():
            for sensor_csv in nodes.values():
                sensor_csv.close()


if __name__ == '__main__':
    parser = argparse.ArgumentParser(description='TDF listener')
    parser.add_argument('--host', dest='base_host', type=str, default="localhost", help='Hostname for baselisten')
    parser.add_argument('--port', dest='base_port', type=int, default=9001, help='Port for baselisten')
    parser.add_argument('--tdf', dest='tdf_server', type=str, default=None, help='Hostname for TDF server')
    parser.add_argument('--path', '-p', dest='output_path', type=str, default='./', help='Output file path')
    parser.add_argument('--date', '-f', dest='date_format', type=str, default="%Y-%m-%dT%H:%M:%S.%f", help='Datetime format')
    parser.add_argument('--debug', dest='debug', action="store_true", help='TDF Debug')
    parser.add_argument('--filter', dest='filter_config', type=str, default=None, help='Filter configuration file')
    parser.add_argument('--strict', dest='filter_strict', action="store_true", help="Don't display unsure devices")

    args = parser.parse_args()

    tdf3csv(**vars(args))

